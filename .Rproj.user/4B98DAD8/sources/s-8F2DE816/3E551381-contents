---
title: "Sample size justification and budget"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(pwr)
library(dplyr)
library(knitr)
```

## Aim 1: Meta-Analysis

```{r, echo = FALSE, results = "hide"}
# Distiller licenses are $75/month for academics
# https://www.evidencepartners.com/pricing/student-pricing/
ds.months = 3
ds.dollars.per.month = 75
# licenses for 6 data-extractors
ds.n.licenses = 6
ds.cost = ds.months * ds.dollars.per.month 

# QSU salary
# 30% effort for 1 month is about 100 person-hours
QSU.fte = 0.30
QSU.months = 1
fringe.perc = .3
QSU.salary = 12000 * 1.03  # per Niraj's budget (inflated by standard 3%)
QSU.pay = ( QSU.fte * (QSU.months/12) ) * QSU.salary * (1 + fringe.perc)

# PI salary
MM.fte = 0.2
MM.salary = 188000 * 1.04  # per Niraj's budget
total.months = 12
MM.pay = ( MM.fte * (total.months/12) ) * MM.salary * (1 + fringe.perc)

# make cost table
t = data.frame( Item = character(0),
                Cost = numeric(0) )

t = add_row(t, Item = "DistillerSR licenses", Cost = ds.cost )
t = add_row(t, Item = "Journal APC", Cost = 2500 )
t = add_row(t, Item = "QSU statistician(s)", Cost = QSU.pay )
t = add_row(t, Item = "PI salary", Cost = MM.pay )
# per Open Phil website, max is 10%
# https://www.openphilanthropy.org/indirect-costs-policy
t = add_row(t, Item = "IDCs (10% max)", Cost = sum(t$Cost) * .10 )
t = add_row(t, Item = "Total", Cost = sum(t$Cost) )
```

```{r, echo = FALSE} 
kable(t)
```

## Aim 2: Pick the Winner

```{r, echo = FALSE, results = "hide"}
# calculate sample size
# for comparison, mean effect size in AWR meta-analysis was very roughly 0.10
#  on Cohen's d scale, conservatively treating OR = RR
log(1.22) * sqrt(3)/pi

# assume a full-factorial design in which each intervention component is 
#  allocated to half the subjects, so power to detect any pairwise difference is like a 
#  2-group trial with half the subjects
# NOT trying to detect interactions here
d = 0.15  # effect size to detect for any main effect comparison
most.levels = 4  # the most levels of any variable
retention = .85 # at one week (only F/U point for this one)
n.all.interv = 5000 * retention  # total subjects getting any intervention (vs. control) AFTER attrition
power.main.effects = pwr.t2n.test( n1 = n.all.interv / most.levels, # the lowest-powered main effect 
                                   n2 = n.all.interv / most.levels, 
                                   d = d )


# calculate subject compensations
ffq.hrs = .5  # hours required to complete FFQ
T0.hrs = 15/60  # hours required to complete all baseline measures and complete intervention
pay.rate = 6  # dollars per hour; see Chandler slide deck
bonus.amount = 1  # additional dollars beyond pay rate above for F/U time point
n.intervs = 4 * 4 * 2 * 2 # number of experimental groups, not including control
# e.g. (animals, environment, health, all) * (no request, reduce, go vegetarian, go vegan) * (graphic vs. not) * (disgusting vs. not)
# note that increasing the number of components doesn't increase the required sample size since 
#  we're only trying to detect MAIN effects; all that matters is the max number of levels of any
#  of the variables

N.cntrl = 800 * retention  # AFTER attrition
N.tot = n.all.interv + N.cntrl # total sample size AFTER attrition

# how wide will CI be for all interventions combined?
# on Cohen's d scale
library(MetaUtility)
var.d = ( ( n.all.interv + N.cntrl ) / ( n.all.interv * N.cntrl ) ) + ( d^2 / ( 2 * (n.all.interv + N.cntrl) ) )
ci.half.width = qnorm(.975) * sqrt(var.d)  # acceptable, I think


# at T0, they spend T0 hrs
# at T1, they spend ffq.hrs (assume 100% retention in terms of payments for conservatism)
total.dollars = ( N.tot / retention ) * ( ( T0.hrs * pay.rate ) + ( ffq.hrs * pay.rate + bonus.amount ) )

# QSU salary
QSU.fte = 0.10
QSU.months = 6
fringe.perc = .3
QSU.salary = 120000 * 1.03
QSU.pay = ( QSU.fte * (QSU.months/12) ) * QSU.salary * (1 + fringe.perc)

# intervention designer salary
designer.fte = 0.10
designer.months = 2
fringe.perc = .3
designer.salary = 60000
designer.pay = ( designer.fte * (designer.months/12) ) * designer.salary * (1 + fringe.perc)

# PI salary
MM.fte = 0.2
MM.salary = 188000 * 1.04 
MM.months = 12
MM.pay = ( MM.fte * (MM.months/12) ) * MM.salary * (1 + fringe.perc)

# make cost table
t = data.frame( Item = character(0),
                Cost = numeric(0) )

t = add_row(t, Item = "Subject payments", Cost = total.dollars )
t = add_row(t, Item = "Journal APC", Cost = 2500 )
t = add_row(t, Item = "QSU statistician", Cost = QSU.pay )
t = add_row(t, Item = "Graphic designer", Cost = designer.pay )
t = add_row(t, Item = "PI salary", Cost = MM.pay )
# per Open Phil website, max is 10%
# https://www.openphilanthropy.org/indirect-costs-policy
t = add_row(t, Item = "IDCs (10% max)", Cost = sum(t$Cost) * .10 )
t = add_row(t, Item = "Total", Cost = sum(t$Cost) )
```

### Sample size calculation

Assume we will conduct a full-factorial design with 4 factors each with a maximum of 4 levels (e.g., request type: no request, "reduce", "go vegetarian", "go vegan"), and that primary analyses seek to detect main effects of each component vs. the other components rather than their potential interactions. Assume the baseline questionnaire (e.g., demographic questions) and intervention together take 5 minutes and that the follow-up questionnaire at 1 week (e.g., the FFQ) takes 30 minutes and has a retention rate of 85%. Then having `r n.all.interv` subjects post-attrition in a balanced allocation across all interventions, excluding the control group, yields approximately `r 100*round(power.main.effects$power, 2)`% power to detect main effects of Cohen's d=`r d` with a two-sided t-test at $\alpha=0.05$.

As a secondary analysis, we will calculate an initial estimate of the effect size of all interventions combined vs. a no-intervention control group including `r N.cntrl` after attrition. If the interventions' average effect size is again Cohen's d=`r d`, this will yield an estimated confidence interval half-width on the Cohen's d scale of `r round(ci.half.width, 2)`. 

Thus, our target sample size to recruit prior to attrition and aggregating across all interventions and the control group, is __n=`r (n.all.interv + N.cntrl)/retention`__.


### Personnel

We plan to hire a PhD-level statistician at the Quantitative Sciences Unit to manage questionnaire development, data collection, and to conduct statistical analyses. Their role will be 10% FTE at a salary of \$120,000/year, with 30% fringe benefits, for 6 months. Additionally, the PI will allocate 20% FTE, at a salary of \$188,000/year, with 30% fringe benefits, for 6 months. 

[In discussion with JP: Do we need to hire someone to help design the interventions, or can his staff do this?]


### Budget

We anticipate the following total costs:
```{r, echo = FALSE} 
kable(t)
```


## Aim 3: MTurk Validation Study


```{r, echo = FALSE, results = "hide"}
# assume a full-factorial design in which each intervention component is 
#  allocated to half the subjects, so power to detect any pairwise difference is like a 
#  2-group trial with half the subjects
# NOT trying to detect interactions here
rm(list=ls())
d = 0.07  # effect size to detect for any main effect comparison
N.recruit = 10000  # total subjects getting intervention or control (BEFORE attrition)
retention = c(0.85, .8, .75)
ni.T3 = N.recruit * min(retention) / 2  # N at final time point in EACH group
# power at the final time point
power = pwr.t2n.test( n1 = ni.T3,  # the lowest-powered main effect 
              n2 = ni.T3, 
              d = d )

# how wide will CI be at the final time point?
# on Cohen's d scale
var.d = ( (2 * ni.T3) / (ni.T3^2) ) + ( d^2 / ( 2 * 2 * ni.T3 ) )
ci.half.width = qnorm(.975) * sqrt(var.d)  # acceptable, I think


# calculate subject compensations
ffq.hrs = .5  # hours required to complete FFQ
demographics.hrs = 5/60  # hours required to complete all baseline measures
pay.rate = 6  # dollars per hour; see Chandler slide deck
bonus.amount = c(0, 1, 3)  # additional dollars beyond pay rate above for T1, T2, T3

# number retained at each time point
Ns.retained = N.recruit * retention

subject.pay = sum( N.recruit * ( demographics.hrs * pay.rate ) +
  Ns.retained * ( ffq.hrs * pay.rate + bonus.amount ) )


##### Other Costs #####
# QSU salary (put together the Qualtrics and manage data collection)
QSU.fte = 0.10
QSU.months = 6
fringe.perc = .3
QSU.salary = 120000 * 1.03
QSU.pay = ( QSU.fte * (QSU.months/12) ) * QSU.salary * (1 + fringe.perc)

# PI salary
MM.fte = 0.2
MM.salary = 188000 * 1.04
MM.months = 12
MM.pay = ( MM.fte * (MM.months/12) ) * MM.salary * (1 + fringe.perc)

# make cost table
t = data.frame( Item = character(0),
                Cost = numeric(0) )

t = add_row(t, Item = "Subject payments", Cost = subject.pay )
t = add_row(t, Item = "Journal APC", Cost = 2500 )
t = add_row(t, Item = "QSU statistician", Cost = QSU.pay )
t = add_row(t, Item = "PI salary", Cost = MM.pay )
# per Open Phil website, max is 10%
# https://www.openphilanthropy.org/indirect-costs-policy
t = add_row(t, Item = "IDCs (10% max)", Cost = sum(t$Cost) * .10 )
t = add_row(t, Item = "Total", Cost = sum(t$Cost) )
```

```{r, echo = FALSE} 
kable(t)
```

### Sample size calculation

Assume we will conduct a simple 2-arm design. Again assume the baseline questionnaire (e.g., demographic questions) and intervention together take 5 minutes and the 3 follow-up questionnaires (e.g., the FFQ) each take 30 minutes. Assume retention rates at the 3 follow-up time points are 85%, 80%, and 75% respectively.    


Then having `r sum(Ns.retained)` subjects post-attrition in a balanced allocation between the intervention and control group yields approximately `r 100*round(power$power, 2)`% power to detect an effect size at the final time point of Cohen's d=`r d` with a two-sided t-test at $\alpha=0.05$.=

Thus, our target sample size to recruit prior to attrition and aggregating across all interventions and the control group, is __n=`r N.recruit`__.


