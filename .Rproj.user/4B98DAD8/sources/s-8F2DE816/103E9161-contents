

#################################### READ IN STEP-1 DATA #################################### 

library(readxl)
library(MetaUtility)
library(metafor)
library(dplyr)
library(data.table)
library(stringr)
library(robumeta)

rerun.sapb.stats = FALSE

code.dir = "~/Dropbox/Personal computer/Independent studies/Sensitivity analysis for publication bias (SAPB)/Linked to OSF (SAPB)/Empirical benchmarks/Code (on git)"
setwd(code.dir)
source("data_prep_helper.R")


data.dir = "~/Dropbox/Personal computer/Independent studies/Sensitivity analysis for publication bias (SAPB)/Linked to OSF (SAPB)/Empirical benchmarks/Data collection"
setwd(data.dir)
setwd("Prepped data for analysis")
b = read.csv("b_data_prepped_step1.csv")


##### Also read in "Study characteristics" dataset #####

setwd(data.dir)
setwd("Study characteristics and author contact")
d.char = read_xlsx("All journals study characteristics.xlsx")

# read in each sheet
keeper.vars = c("Exposure (with largest sub-meta-analysis)",
                "Outcome (with largest sub-meta-analysis)",
                "Reported pooled estimate",
                "Reported CI hi",
                "Refid",
                "Data source")

for (i in 1:3) {
  chunk = read_xlsx("All journals study characteristics.xlsx", sheet = i)
  
  # don't include info for metas that weren't actually analyzed
  chunk = chunk[ !is.na(chunk$Coders), keeper.vars ]
  
  if (i == 1) d.char = chunk
  else d.char = rbind(d.char, chunk)
}
# for merging joy
d.char$Refid = tolower(d.char$Refid)


#################################### STEP 2A: FILL IN EACH STUDY'S STATS #################################### 

# initialize the new variables to use in analysis
# using uppercase variable names for the estimates on the analysis scale
b$SE = NA
b$Est = NA
b$Analysis.Scale = NA

# look at the effect measure used in each study
table(b$effect.measure)


##### Calculate analysis-ready estimate and SE for every row #####
temp.list = apply( b, 
                    1, 
                    fill_row_stats )
b = rbindlist( temp.list )


# check for problems
#table(b$Est==0, b$Analysis.Scale)  # should never happen since we add jitter when it does
table(b$SE==Inf)
table( is.na(b$Est), is.na(b$SE) )

# convert estimates to numeric
b$Est = as.numeric( as.character(b$Est) )
b$SE = as.numeric( as.character(b$SE) )


##### Fix SEs equal to 0 or Inf #####

# sometimes due to rounding, a study has 0 variance
# this breaks meta-analysis estimation
# in these cases, set to a small value

# all of these are also studies with point estimates of exactly 0
# adjust SE to be equal to 1/10 of the point estimate itself in these cases
table(b$SE == 0)
b$SE[ eq(b$SE, 0) ] = abs( b$Est[ eq(b$SE, 0) ] / 10 )


# some of the "Inf" ones are for correlations with n=3 exactly
# or RR = 0
#View(b[b$SE > 0 & b$SE < Inf,])
table( b$meta.name[ b$SE <= 0 | b$SE == Inf ] )
b = b[b$SE > 0 & b$SE < Inf,]


# save intermediate dataset for debugging
setwd(data.dir)
setwd("Prepped data for analysis")
write.csv(b, "b_data_step2a.csv", row.names = FALSE)


#################################### STEP 2B: RE-META-ANALYZE #################################### 

# using all point estimates from each study for this (this is just a sanity check to approximately reproduce
#  published estimates)
# though note that we don't expect to exactly reproduce the reported estimates since we're excluding unpublished
#  studies
temp = b %>% group_by(meta.name, coder) %>%
  do( meta_stats(.) )

# put the point estimates back in big dataset
b = merge( b, temp, by = c("meta.name","coder"))

# merge in the data from the "Study characteristics" dataframe
# "all.x" because Metalab doesn't have this info in d.char dataset, so 
#  we don't want to lose its rows
b = merge( b, d.char, by.x = "meta.name", by.y = "Refid", all.x = TRUE )

# save intermediate dataset for debugging
setwd(data.dir)
setwd("Prepped data for analysis")
write.csv(b, "b_data_step2b.csv", row.names = FALSE)



#################################### STEP 2C: FIT SELECTION MODEL - ALL ESTIMATES #################################### 

setwd(data.dir)
setwd("Prepped data for analysis")
b = read.csv("b_data_step2b.csv")

# IMPORTANT: IN THIS VERSION, WE'RE INCLUDING ALL POINT ESTIMATES TO ENABLE INTERRATER
#  COMPARISONS. WE WILL THEN FIT THE SELECTION MODEL AGAIN AFTER RANDOMLY CHOOSING ONE
#  ESTIMATE PER STUDY. 

# sanity check: should have one estimate per study.name and meta.name
length(unique(b$study.name))

##### Flip The Point Estimate Signs As Needed #####

# important: we flip the signs based on a naive REML
#  meta-analysis of all point estimates, similar to what most authors did 
#  in the literature
# "EstF" stands for "estimate flipped"
# ~~~ BOOKMARK
b$EstF = b$Est
b$EstF[ b$Mhat < 0 ] = -b$Est[ b$Mhat < 0 ]

# should never happen!
which(!b$Est == 0 & b$Est == b$EstF & b$Mhat < 0)


##### Calculate P-Values #####
# one-tailed p-value
b$Pval.One = ( 1 - pnorm( ( b$EstF / b$SE ) ) )
# two-tailed
b$Pval.Two = 2 * ( 1 - pnorm( abs(b$EstF / b$SE) ) )
b$signif = b$Pval.Two < 0.05
b$signif[ b$signif == TRUE] = 1  # for eventual geom_smooth joy
b$signif[ b$signif == FALSE] = 0

b$affirm = b$signif == TRUE & b$EstF > 0

##### Fit Selection Models #####
temp2 = b %>% group_by(meta.name, coder) %>%
  do( selection_stats(.) )

# put the point estimates back in big dataset
b = merge( b, temp2, by = c("meta.name","coder"))

# save intermediate dataset for debugging
setwd(data.dir)
setwd("Prepped data for analysis")
write.csv(b, "b_data_step2c.csv", row.names = FALSE)


#################################### INTERRATER DISCREPANCIES #################################### 

# read in intermediate dataset
setwd(data.dir)
setwd("Prepped data for analysis")
b = read.csv("b_data_step2c.csv")

# put coder info in dataset
b = b %>% group_by(meta.name) %>% mutate( n.coders = n_distinct(coder),
                                              coders = paste( unique(coder), collapse = " " ) )

# look at agreement between our estimates and the reported ones
# sc1 = "sanity check 1"
sc1 = b %>% group_by(meta.name, coder) %>%
  summarise( group = group[1],
            Mhat = Mhat[1],
             Mhat.hi = Mhat.hi[1], 
             Eta = Eta[1],
             LogEta = LogEta[1],
             VarLogEta = VarLogEta[1],
             Mhat.reported = as.numeric( as.character(Reported.pooled.estimate[1]) ),
             Mhat.hi.reported = as.numeric( as.character(Reported.CI.hi[1] ) ),
             k.coded = n(),
            n.coders = n.coders[1],
            coders = coders[1] )

# percent discrepancies between our pooled point estimate and paper's reported estimate
sc1$Mhat.vs.reported = perc_diff( sc1$Mhat, sc1$Mhat.reported )
sc1$Mhat.hi.vs.reported = perc_diff( sc1$Mhat.hi, sc1$Mhat.hi.reported )
sc1 = sc1[ order(sc1$n.coders, sc1$coders, sc1$Mhat.vs.reported, decreasing = TRUE), ]

# agreement between multiple coders' pooled point estimates
# wide meta stats
wide.vars = c(
              "Mhat",
              "Mhat.hi",
              "Mhat.vs.reported",
              "Mhat.hi.vs.reported",
              "Eta",                                    
              "LogEta",                                   
              "VarLogEta",
              "k.coded")
msw = as.data.frame( dcast(setDT(sc1),
                           meta.name + coders + n.coders + group ~ coder,
                           value.var = wide.vars) )

# overall agreement within each coding team
msw = make_coder_agreement_var("Mhat", .dat = msw, percent = TRUE)
msw = make_coder_agreement_var("Mhat.hi", .dat = msw, percent = TRUE)
msw = make_coder_agreement_var("Eta", .dat = msw, percent = TRUE)
msw = make_coder_agreement_var("LogEta", .dat = msw, percent = TRUE)
msw = make_coder_agreement_var("VarLogEta", .dat = msw, percent = TRUE)
msw = make_coder_agreement_var("k.coded", .dat = msw, percent = FALSE)

# status of each meta-analysis
msw$max.discrep = pmax( abs(msw$LogEta.disagreement),
                        abs(msw$VarLogEta.disagreement),
                        abs(msw$Mhat.disagreement) ) 

msw$status[ msw$n.coders == 1 ] = "singly-coded"
msw$status[ msw$max.discrep > 5 ] = "discrepant"
msw$status[ msw$max.discrep == 0 ] = "perfect agreement"
msw$status[ msw$max.discrep > 0 & msw$max.discrep <= 5 ] = "slightly discrepant"
msw$status[ msw$discrepant == FALSE ] = "use"

# **show statuses by group - for PRISMA
table(msw$status, msw$group, useNA = "ifany")
# ** proportion of doubly-coded metas that were at most slightly discrepant: 64%
msw %>% filter( status != c("singly-coded") ) %>%
  filter( group != "metalab.sens") %>%
  summarise( mean( status != c("discrepant") ) )


cols.to.show = c("meta.name",
                 "coders",
                 names(msw)[grepl("disagreement", names(msw))],
                 names(msw)[grepl("k.coded", names(msw))])
#View(msw[ , cols.to.show])


setwd(data.dir)
setwd("Prepped data for analysis")
write.csv(msw, "msw_discrepancies.csv", row.names = FALSE)


#################################### STEP 2D: INVESTIGATE SPECIFIC DISCREPANCIES #################################### 

# # resolve specific discrepancies for a given meta
# 
# # figure out which coders worked on the meta
# table(b$coder[b$meta.name=="25216101"])
# 
# res= find_num_discrep( coders = c("mbm", "lm"),
#                      meta.name = "24622676",
#                      dat = b,
#                      sort = FALSE,
#                      col.name = "point.est")  # point.est, se, ci.hi, etc.
# res$lst

#################################### RESOLVE SOME CONFLICTS BY CHOOSING ONE CODER'S DATASET #################################### 

# merge the status indicator into the long dataset
b = merge(b,
          msw[, c( "status", "meta.name" ) ],
          all.x = TRUE,
          by = "meta.name" )

# indicator for whether we will use a given row of data in analysis
b$use = FALSE
b$use[ b$group == "metalab" ] = TRUE
b$use[ b$group == "metalab.sens" ] = TRUE


##### Metas with Perfect-Agreement Ones and Slightly Discrepant Ones (0% < Max Discrep <= 5%)  #####

# just choose the first dataset since they're almost the same
# (though not necessarily wrt the journals)
( good.metas = unique( b$meta.name[b$status %in% c("perfect agreement", "slightly discrepant") ] ) )


table(b$coder[b$meta.name == good.metas[1]])

# has same length as b
first.coder = ( b %>% group_by(meta.name) %>% 
  mutate( first = unique(coder)[1] ) )$first

b$use[ b$meta.name %in% good.metas & 
         b$coder == first.coder ] = TRUE

b$use[ b$meta.name %in% good.metas & 
         b$coder != first.coder ] = FALSE

# sanity check
table( b$coder[ b$meta.name == good.metas[1] ],
       b$use[ b$meta.name == good.metas[1] ] )


##### Discrepant Ones (Max Discrep > 5%) #####

table(b$coder[b$meta.name=="pb_5"])

# MBM had prepped raw data in R
b = supersede( dat = b,
                  meta.name = "27835651",
                  coder = "mbm" )

# MBM had prepped raw data in R
b = supersede( dat = b,
               meta.name = "25365303",
               coder = "mbm" )

# two sign errors in MBM's CI limits (confirmed against paper's forest plot)
b = supersede( dat = b,
               meta.name = "29293522",
               coder = "tb" )

# not sure who was "right", but I manually checked every entry in MM's, so used his 
b = supersede( dat = b,
               meta.name = "pb_5",
               coder = "mm" )

# the other coder had entered the pooled point estimate in place of the last entry
b = supersede( dat = b,
               meta.name = "24959902",
               coder = "lm" )

# MBM had prepped raw data in R
b = supersede( dat = b,
               meta.name = "29474359",
               coder = "mbm" )

# the other coder forgot to enter the last point estimate
b = supersede( dat = b,
               meta.name = "25409023",
               coder = "lm" )

# used the author-provided dataset to confirm the correct number of published studies, 
#  which matches CP's dataset
b = supersede( dat = b,
               meta.name = "pb_19",
               coder = "cp" )

# the other coder made several confusing errors here
b = supersede( dat = b,
               meta.name = "lancet_12",
               coder = "cp" )

# MBM had prepped raw data in R
# and she exactly reproduced the authors' stats
b = supersede( dat = b,
               meta.name = "24865979",
               coder = "mbm" )

# MBM had prepped raw data in R
# though couldn't reproduce authors' stats
b = supersede( dat = b,
               meta.name = "25216101",
               coder = "mbm" )

# MBM had prepped raw data in R
# and she exactly reproduced the authors' stats
b = supersede( dat = b,
               meta.name = "27416099",
               coder = "mbm" )

# MBM had prepped raw data in R
# though couldn't reproduce authors' stats
b = supersede( dat = b,
               meta.name = "25141289",
               coder = "mbm" )

# other coder had left out a few negative signs in CI limits
# checked against paper's forest plot
b = supersede( dat = b,
               meta.name = "24968234",
               coder = "mbm" )

# this paper had a special way of calculating variances since effect measure was a response ratio, 
#  and MBM coder had used raw data to reproduce this while the other had entered CI limits
#  such that we used the Wald approximation to get variances
b = supersede( dat = b,
               meta.name = "28700609",
               coder = "mbm" )

# this one required a lot of digging and correspondence with authors to get the full citations
#  CP had gone through this very carefully
b = supersede( dat = b,
               meta.name = "ps_1",
               coder = "cp" )

# this one had 3 coders
# MBM and LM had exactly the same pooled point estimate and CI limit,
#  so use one of theirs
b = supersede( dat = b,
               meta.name = "25111792",
               coder = "mbm" )

# exactly same situation as 25111792 above
b = supersede( dat = b,
               meta.name = "29408858",
               coder = "mbm" )

# the other coder forgot to scrape data from one subpart of a multi-part 
#  forest plot
b = supersede( dat = b,
               meta.name = "24622676",
               coder = "lm" )

# this one actually did have raw data, but CP had carefully coded all the journals
#  and found that some studies had unclear journal information (e.g., they were marked 
#  as "published" in the public dataset, but weren't in reference list)
b = supersede( dat = b,
               meta.name = "pb_16",
               coder = "cp" )

# the following ones did not have discrepancies, but I'm choosing CP's over mine
#  because she entered the journals and years whereas I did not
b = supersede( dat = b,
               meta.name = "pb_13",
               coder = "cp" )

b = supersede( dat = b,
               meta.name = "pb_2",
               coder = "cp" )

b = supersede( dat = b,
               meta.name = "pb_9",
               coder = "cp" )

b = supersede( dat = b,
               meta.name = "pps_1",
               coder = "cp" )



##### Unresolvable Ambiguities (Exclude Completely) #####

# do not supersede because we don't want it included
# lancet_11 - this one used trial acronyms in the forest plot, and we can't tell which are published
#   or unpublished
# lancet_9 - most estimates are from large public databases rather than publications (or if they are
#   from publications, there is no indication of what publication they would be from).
b$status[ b$meta.name == "lancet_11" ] = "unresolvable (excluded)"
b$status[ b$meta.name == "lancet_9" ] = "unresolvable (excluded)"

#table(b$coder[b$meta.name=="ps_1"])


##### Singly-Coded #####
# the following were singly-coded, but I confirmed that it would 
#  be a pain to code them myself: 
# - pb_15: has public data, but would need to manually code 186 studies as published vs. unpublished
# - pb_21: same issue (AL half-coded this one and then stopped)
# - pb_14: has dataset in huge, messy Word table
# - 25360660: would not be hard to dual-code (smallish forest plot), but I was already the single coder


##### Look At Which Metas Were Included and Excluded #####
# sanity check
table(b$meta.name, b$use)
# many have same number in the use==FALSE and use==TRUE columns, which makes sense:
#  that's because we're only using one coder's data

# look for metas that are completely excluded
# we don't care about the ones that are singly-coded; those are supposed to be excluded
View( b %>% group_by(meta.name) %>%
        #filter(status != "singly-coded") %>%
        mutate( never.use = all(use==FALSE) ) %>%
        filter(never.use == TRUE) %>%
        summarise(n(), 
                  status[1])
)

# **for PRISMA: how many did we extract data from? 69
b %>% 
  filter( !is.na(meta.name) & group != "metalab.sens") %>%
  group_by(group) %>%
  summarise(k = length(unique(meta.name)))

# **for PRISMA: how many are to be analyzed? 63
b %>% filter(use==TRUE) %>%
  filter( !is.na(meta.name) & group != "metalab.sens") %>%
  group_by(group) %>%
  summarise(k = length(unique(meta.name)))

# **for PRISMA: analyzed metas, broken down by meta.journal
b %>% filter(use==TRUE) %>%
  filter( !is.na(meta.name) & group %in% c("top.med", "top.psych") ) %>%
  group_by(meta.journal) %>%
  summarise(k = length(unique(meta.name)))

# save intermediate dataset for debugging
setwd(data.dir)
setwd("Prepped data for analysis")
write.csv(b, "b_data_step2d.csv", row.names = FALSE)

# look at missing data on year and journal
# # ~~~ bm
# View( b %>% filter( use == TRUE ) %>%
#         group_by(meta.name) %>%
#         summarise( Pmiss.year = mean(is.na(study.year)),
#                    Pmiss.journal = mean(is.na(study.journal)),
#                    coder = coder[1]) %>%
#         arrange(Pmiss.journal) )
# 
# mean(is.na(b$study.year[b$group %in% c("top.psych", "top.med")]))
# mean(is.na(b$study.journal[b$group %in% c("top.psych", "top.med")]))
# mean(is.na(b$first2[b$group %in% c("top.psych", "top.med")]))


#################################### STEP 2E: FIT SELECTION MODEL AGAIN - NOW WITH ONLY RANDOMLY CHOSEN ESTIMATES #################################### 


setwd(data.dir)
setwd("Prepped data for analysis")
b = read.csv("b_data_step2d.csv")

##### Randomly Choose One Point Estimate Per Study ######
# added grouping by coder because otherwise when coders enter the same study name, 
#  the function will drop too many point estimates
b = b %>% group_by(meta.name, study.name, coder) %>%
  mutate( randomly.chosen = my_sample( n() ) )
table(b$randomly.chosen[b$meta.name == "pb_17"])

# # sanity check
# table( b$randomly.chosen[ b$meta.name == "24586827" & b$coder == "mbm" ] )
# View( b[ b$meta.name == b$meta.name[1], c("study.name", "randomly.chosen") ] )

# 64% of the point estimates are duplicated within studies! 
prop.table(table(b$randomly.chosen))


##### Fit Selection Models Again ######

# note that selection_stats fn automatically adds variables with fixed names, 
#  so overwrites the previous results using all point estimates 
temp2 = b %>% group_by(meta.name, coder) %>%
  do( selection_stats(.,
                      only.randomly.chosen = TRUE ) )

# put the point estimates back in big dataset
b = merge( b, temp2, by = c("meta.name","coder"))


# since the selection_stats fn automatically adds variables with fixed names, 
# rename them to differentiate
to.replace = grepl( ".x", names(b) ) &
  ( grepl( "Eta", names(b) ) | grepl( "shapiro", names(b) ) )
names(b)[to.replace]
names(b)[to.replace] = str_replace( string = names(b)[to.replace],
                                    "[.]x",
                                    ".AllEsts" )

# the main estimates to meta-analyze should not have any suffix
to.replace = grepl( ".y", names(b) ) &
  ( grepl( "Eta", names(b) ) | grepl( "shapiro", names(b) ) )
names(b)[to.replace]
names(b)[to.replace] = str_replace( string = names(b)[to.replace],
                                    "[.]y",
                                    "" )


# save intermediate dataset for debugging
setwd(data.dir)
setwd("Prepped data for analysis")
write.csv(b, "b_data_step2e.csv", row.names = FALSE)


#################################### STEP 2F: MAKE NEW VARIABLES #################################### 

setwd(data.dir)
setwd("Prepped data for analysis")
b = read.csv("b_data_step2e.csv")

# change factor levels
levels( b$group )
b$group = factor( b$group,
                   levels = c("plos", "top.med", "top.psych", "metalab", "metalab.sens") )
table(b$group, useNA = "ifany")

# for plotting joy
b$group.pretty[ b$group == "plos" ] = "PLOS One"
b$group.pretty[ b$group == "top.med" ] = "Top medical"
b$group.pretty[ b$group == "top.psych" ] = "Top psychology"
b$group.pretty[ b$group == "metalab" ] = "Metalab"
b$group.pretty[ b$group == "metalab.sens" ] = "Metalab (sens.)"
b$group.pretty = factor( b$group.pretty,
                          levels = c("PLOS One", "Top medical", "Top psychology", "Metalab", "Metalab (sens.)") )

# **look at usable ones by group (for PRISMA)
b %>% 
  filter( !is.na(group) & group != "metalab.sens") %>%
  group_by(use, group.pretty) %>%
  summarise(length(unique(meta.name)))
# ** look at statuses (for PRISMA)
b %>% 
  filter( !is.na(group) & group != "metalab.sens") %>%
  group_by(status, group.pretty) %>%
  summarise(k=length(unique(meta.name)))

# make version with only the metas to analyze
b2 = b[ b$use == TRUE,]
table(b2$meta.name, b2$use)
length(unique(b2$meta.name))


#################################### STEP 2F: MAKE ANALYSIS DATASET (NO DUAL-CODING) #################################### 

# NO LONGER NEEDED?
# # recode group variable
# b2$group = factor( b2$group,
#                    levels = c("plos", "top.med", "top.psych", "metalab", "metalab.sens") )
# b2$group.pretty = factor( b2$group.pretty,
#                           levels = c("PLOS One", "Top medical", "Top psychology", "Metalab") )
# 
# # fix factor levels again
# b$group = factor( b$group,
#                   levels = c("plos", "top.med", "top.psych", "metalab") )


# percentile of estimate within meta-analysis
my_ecdf = function(vec, value) {
  if ( all( is.na(vec) ) ) return(NA)
  ecdf(vec)(value)
}
b2 = b2 %>% group_by(meta.name) %>% 
  mutate( Qi = my_ecdf(EstF,EstF) )


# indicator for whether a given study was one of the first three
#  within each meta
# the "dense" method means that every point estimate published in the same year gets the
#  same rank, with no gaps in the ranks
#  so more than 2 can be "first 2"
b2 = b2 %>% group_by(meta.name) %>%	
  mutate( year.rk = my_frank( study.year ),
          n.first.yr = length( unique( study.name[year.rk == 1] ) ), # how many papers published in first unique year?
          
          # if the number published in first chronological year is >1, then only
          #  studies in that year are "first2"
          # but if only one study was published in the first chronological year, 
          #  then all studies from first TWO chronological years are "first2"
          first2 = ifelse( n.first.yr > 1, year.rk == 1, year.rk <= 2),
          n.first2 = length( unique( study.name[first2 == TRUE] ) ) ) # number of unique papers marked as "first2"
          
table( is.na(b2$year.rk), b2$discipline, useNA="ifany")

       
# # sanity checks
# # e.g., 3 studies all published in 2000 and one published in 2010, each with 2 point estimates
# fake = data.frame( study.name = rep(letters[1:5], each = 2),
#                    study.year = c( rep(2000, 6), 2010, 2010, 2011, 2011) )
# fake %>%  mutate( year.rk = frank( study.year,	
#                                    ties.method = "dense" ),
#                   n.first.yr = length( unique( study.name[year.rk == 1] ) ), 
#                   first2 = ifelse( n.first.yr > 1, year.rk == 1, year.rk <= 2),
#                   n.first2 = length( unique( study.name[first2 == TRUE] ) ) )
# 
# # e.g., one study published in 2000 and three published in 2011
# fake = data.frame( study.name = rep(letters[1:5], each = 2),
#                    study.year = c( rep(2000, 2), rep(2011, 8) ) )
# fake %>%  mutate( year.rk = frank( study.year,	
#                                    ties.method = "dense" ),
#                   n.first.yr = length( unique( study.name[year.rk == 1] ) ), 
#                   first2 = ifelse( n.first.yr > 1, year.rk == 1, year.rk <= 2),
#                   n.first2 = length( unique( study.name[first2 == TRUE] ) ) )
# 
# 
# # sanity check	
# temp = b2[ b2$meta.name == b2$meta.name[1], ]	
# temp = temp[ order(temp$study.year), ]	
# cbind( temp$study.year,	
#        temp$year.rk,	
#        temp$first2,
#        temp$n.first2)[1:30,]
# 
# b2 = b2[!is.na(b2$meta.name),]

# remove useless columns
b2 = b2[ , !names(b2) %in% c("X.2", "X", "X.1", "X.x", "X.y") ]


# save intermediate dataset for debugging
setwd(data.dir)
setwd("Prepped data for analysis")
write.csv(b2, "b2_data_step2f.csv", row.names = FALSE)



#################################### STEP 2F.b: FIT SELECTION MODELS TO HIF VS. LIF SUBSETS WITHIN SOME METAS #################################### 

setwd(data.dir)
setwd("Prepped data for analysis")
b2 = read.csv("b2_data_step2f.csv")


# ~~~
# bm: 2019-12-7
# try to just make a separate dataset for the hif vs. lif estimates
b3 = b2 %>% filter( group %in% c("top.psych", "top.med") ) %>%
  filter( randomly.chosen == TRUE ) %>%
  filter( !is.na(hif) ) %>%
  group_by(meta.name) %>% 
  mutate( n.hif = sum(hif),
          n.lif = sum(1-hif),
          k.affirm.hif = sum( affirm[ hif ==  TRUE ] ),
          k.nonaffirm.hif = sum( !affirm[ hif == TRUE ] ),
          k.affirm.lif = sum( affirm[ hif == FALSE ] ),
          k.nonaffirm.lif = sum( !affirm[ hif == FALSE ] ) ) %>%
  filter(n.hif >= 40 &  # going down to 30 here does not help
           n.lif >= 40 &
           k.affirm.hif >= 3 &
           k.nonaffirm.hif >= 3 &
           k.affirm.lif >= 3 & 
           k.nonaffirm.lif >= 3 )

b3 = droplevels(b3)
table(b3$meta.name, b3$coder)


##### Fit Selection Models #####
# don't need to group by coder this time since we've kept only 1 coder's data
#  per meta
temp3 = b3 %>%
  filter( !is.na(hif) ) %>%
  group_by( meta.name, hif ) %>%
  do( selection_stats( .,
                      only.randomly.chosen = TRUE ) ) 

# interesting: seemingly different patterns between these two metas

# look at which HIF journals each one had
# intriguing! That J Educ Psych is the culprit again! 
as.data.frame( b3 %>% filter( hif == TRUE ) %>%
                 group_by(meta.name, study.journal) %>%
                 summarise( n = n() ) )

# info about most-represented journals in each HIF category
temp4 = b3 %>%
  filter( !is.na(hif) ) %>%
  group_by( meta.name, hif ) %>%
  summarise( n = n(),
             perc.hif.jep = round(  100 * mean( study.journal == "journal of educational psychology" ), 0 ),
             perc.hif.jcr.jpsp = round( 100 * mean( study.journal %in% c("journal of consumer research",
                                                                         "journal of personality and social psychology" ) ) ) )

temp3 = merge( temp3, temp4, by = c("meta.name", "hif") )


setwd(data.dir)
setwd("Prepped data for analysis")
write.csv(temp3, "res_within_hif_step2fb.csv", row.names = FALSE)


#################################### STEP 2G: DO SAPB ANALYSES #################################### 

setwd(data.dir)
setwd("Prepped data for analysis")
b2 = read.csv("b2_data_step2f.csv")

##### Do SAPB Analyses ######

# note that this overwrites the Mhat, etc. 
if ( rerun.sapb.stats == TRUE ) {
  temp2 = b2 %>% group_by(meta.name) %>%
    do( sapb_stats(., only.randomly.chosen = FALSE ) )

  setwd(data.dir)
  setwd("Prepped data for analysis")
  write.csv(temp2, "temp2_sapb_stats.csv")
} else {
  setwd(data.dir)
  setwd("Prepped data for analysis")
  temp2 = read.csv("temp2_sapb_stats.csv")
}

# put the point estimates back in big dataset
b2 = merge( b2,
            temp2,
            all.x = TRUE,
            by = c("meta.name"))

# clarify coding
names(b2)[names(b2) == "Mhat.x"] = "Mhat.to.reproduce"  # the pooled pt estimate BEFORE random selection of point ests and without flipping direction
names(b2)[names(b2) == "Mhat.hi"] = "Mhat.hi.to.reproduce"
names(b2)[names(b2) == "Mhat.y"] = "Mhat"  # AFTER random selection of point ests and AFTER flipping point estimate direction

# make numeric version of s-value
b2$Sval.Est.Num = as.numeric(as.character(b2$Sval.Est))
b2$Sval.Est.Num[ b2$Sval.Est == "Not possible" ] = Inf
b2$Sval.Est.Num[ b2$Sval.Est == "> 200" ] = 200

b2$Sval.CI.Num = as.numeric(as.character(b2$Sval.CI))
b2$Sval.CI.Num[ b2$Sval.CI == "Not possible" ] = Inf
b2$Sval.CI.Num[ b2$Sval.CI == "> 200" ] = 200

# save intermediate dataset for debugging
setwd(data.dir)
setwd("Prepped data for analysis")
write.csv(b2, "b2_data_step2g.csv", row.names = FALSE)


#################################### MAKE AGGREGATED DATASET (1 ROW PER META) #################################### 

setwd(data.dir)
setwd("Prepped data for analysis")
b2 = read.csv("b2_data_step2g.csv")

##### Take Existing Summary Variables from b2 Dataset #####
f2 = b2 %>%
  group_by(meta.name) %>% 
  summarise( LogEta = LogEta[1],
             VarLogEta = VarLogEta[1],
             shapiro.pval = as.numeric( as.character( shapiro.pval[1] ) ),
             Mhat = Mhat[1],
             Mhat.Lo = Mhat.Lo[1],
             Mhat.Hi = Mhat.Hi[1],
             Mhat.Pval = Mhat.Pval[1],
             
             Analysis.Scale = Analysis.Scale[1],
             
             # from SAPB analyses
             Sval.Est = Sval.Est[1],
             Sval.CI = Sval.CI[1],
             Sval.Est.Num = Sval.Est.Num[1],
             Sval.CI.Num = Sval.CI.Num[1],
             Mhat.Worst = Mhat.Worst[1],
             Mhat.Worst.Lo = Mhat.Worst.Lo[1],
             Mhat.Worst.Hi = Mhat.Worst.Hi[1],
             Mhat.Worst.Pval = Mhat.Worst.Pval[1],
             
             group = group[1],
             group.pretty = group.pretty[1],
             discipline = discipline[1],
             Data.source = Data.source[1],
             k.all = n(),	
             k.rc = sum(randomly.chosen),	
             k.affirm.rc = sum(randomly.chosen == 1 & affirm == TRUE),	
             k.nonaffirm.rc = sum(randomly.chosen == 1 & affirm == FALSE) )

##### Calculate Within-Meta-Analysis Summary Variables #####
f3 = b2 %>% group_by(meta.name) %>%	

  summarise(
    # top- vs. lower-tier 
    Qi.hif = mean( Qi[hif == 1], na.rm = TRUE ),
    Qi.lif = mean( Qi[hif == 0], na.rm = TRUE ),
    Qi.hif.diff = Qi.hif - Qi.lif,
    
    Psig.hif = mean(signif[hif == 1]),
    Psig.lif = mean(signif[hif == 0]),
    Psig.hif.diff = Psig.hif - Psig.lif,
    Psig.hif.RR = Psig.hif / Psig.lif,	
    k.hif = sum(hif),	
    k.lif = sum(hif == 0),	
    
    # first2 status
    Qi.early = mean( Qi[first2 == 1], na.rm = TRUE ),	
    Qi.late = mean( Qi[first2 == 0], na.rm = TRUE ),	
    Qi.early.diff = Qi.early - Qi.late,	
    Psig.early = mean(Pval.Two[first2 == 1] < 0.05),	
    Psig.late = mean(Pval.Two[first2 == 0] < 0.05),	
    Psig.early.diff = Psig.early - Psig.late, 	
    Psig.early.RR = Psig.early / Psig.late,	
    k.early = sum(first2),	
    k.late = sum(first2==0),
    
    # for sensitivity analyses on our results
    # is there evidence of 2-tailed selection in this meta?
    # first term is the observed % of nonaffirmatives that were significant in wrong direction
    # second term is the expected % if all nonaffirmatives are uniform (interval of length .975)
    # if the nonaffirms aren't uniform (i.e., because there is an effect in the other direction), 
    #  then we'd only expect the ratio to be even smaller
    # so it's conservative
    Pdisaffirm.ratio = mean( Pval.One[!affirm] > 0.975 ) / (0.025 / 0.975),
    
    # for sanity check about point mass at zero
    PZeroEst = mean( Est == 0 )
  )	

##### RRs of Being Affirmative by HIF and First2 #####
# this one has 30 rows (none for PLOS)
f3.RRs.hif = b2 %>% group_by(meta.name) %>%	
  filter( !is.na(hif) ) %>%	
  do(logRR.hif = as.numeric( my_escalc_RR(., X.name = "hif")[1] ),	
     varlogRR.hif = as.numeric( my_escalc_RR(., X.name = "hif")[2] ) )	

# this has all 63 rows
f3.RRs.early = b2 %>% group_by(meta.name) %>%	
  filter( !is.na(first2) ) %>%	
  do(logRR.first2 = as.numeric( my_escalc_RR(., X.name = "first2")[1] ),
     varlogRR.first2 = as.numeric( my_escalc_RR(., X.name = "first2")[2] ) )

#droplevels(f2)

f3 = merge( f3, 	
            f3.RRs.hif, 	
            by = "meta.name",	
            all.x = TRUE )	

f3 = merge( f3,	
            f3.RRs.early,	
            by = "meta.name",	
            all.x = TRUE )	

#f2 = apply(f2, 2, unlist)
f2 = merge( f2,	
            f3,	
            by = "meta.name",	
            all.x = TRUE )	

# goes back to a factor :(
f2$shapiro.pval = as.numeric( as.character(f2$shapiro.pval) )
#f2 = apply(f2, 2, unlist)

# exclusion indicator for sensitivity analyses
f2$excl.Pdisaffirm.ratio = f2$Pdisaffirm.ratio > 3
f2$excl.PZeroEst = f2$PZeroEst > .05
f2$excl.shapiro = f2$shapiro.pval < .05
# ** exclude the one with inconsistent effect direction coding (list in PRISMA)
f2$excl.other = f2$meta.name == "25360660"  

# how many are now usable?
dim(f2)


#################################### MAKE CODEBOOKS #################################### 

##### Label Variables and Make Codebook for b2 Dataset #####
library(Hmisc)
#https://www.statmethods.net/input/variablelables.html
label(b2$meta.name) = "Name of meta-analysis"
label(b2$coder) = "Coder initials from data entry"
label(b2$key.title) = "Standardized title for merging with SJR dataset"
label(b2$group) = "**Group of meta-analysis"
label(b2$discipline) = "For PLOS One, discipline of meta-analysis"
label(b2$meta.journal) = "Journal of meta-analysis"
label(b2$study.name) = "Name of study within meta-analysis, used for randomly selecting point estimates for selection model"
label(b2$study.journal) = "Journal of study within meta-analysis"
label(b2$study.year) = "**Publication year of study within meta-analysis"
label(b2$point.est) = "Point estimate of study prior to transformations and direction-flipping; its effect size is of type described in column effect.measure"
label(b2$effect.measure) = "Scale of point.est (e.g., r, or, hr, log-hr, etc.)"
label(b2$var) = "Variance of point.est"
label(b2$se) = "Standard error of point.est"
label(b2$ci.hi) = "Upper CI limit of point.est"
label(b2$n) = "Sample size of point.est"
label(b2$total.flags) = "All flag variables were just for finding bad data during entry"
label(b2$SJR) = "SJR ranking of study journal"
label(b2$SJR.type) = "Type of publication as listed in SJR database (used to help weed out any remaining unpublished studies)"
label(b2$SJR.hand.coded) = "Was SJR hand-coded (vs. automatically merged from database)?"
label(b2$SJR.topic.cats) = "Study journal's topic categories as listed in SJR database"
label(b2$hif.thresh) = "SJR threshold against which this study was compared (based on whether group was top.med or top.psych)"
label(b2$hif) = "**Is this study top-tier? Missing data occur because we only coded a subset of meta-analyses due to time constaints."
label(b2$Est) = "Study's point estimate after scale transformations but BEFORE direction-flipping if needed"
label(b2$SE) = "Study's standard error as analyzed (i.e., after scale transformations to the scale described in Analysis.Scale if needed)"
label(b2$EstF) = "**Study's point estimate as analyzed (i.e., after scale transformations to the scale described in Analysis.Scale and direction-flipping if needed)"
label(b2$Analysis.Scale) = "What effect measure was used in analysis?"
label(b2$Mhat.to.reproduce) = "Pooled point estimate of meta-analysis ONLY for the purposes of sanity-checking against the meta-analysis'
reported results. This pooled estimate is BEFORE random selection of point estimates and BEFORE flipping point estimate directions, and its uses regular REML model
as fit by the function meta_stats. This estimate has been transformed back to the scale described by effect.measure (e.g., r instead of z) to ease comparison with reported results.
NOT directly comparable with Mhat."
label(b2$Mhat.hi.to.reproduce) = "Upper CI limit of Mhat.to.reproduce. See the important caveats for that variable."
label(b2$Exposure..with.largest.sub.meta.analysis.) = "The exposure or independent variable in the meta-analyzed study"
label(b2$Outcome..with.largest.sub.meta.analysis.) = "The outcome or dependent variable variable in the meta-analyzed study"
label(b2$Reported.pooled.estimate) = "The reported pooled point estimate in the meta-analysis for comparison with Mhat.to.reproduce"
#label(b2$) = "Upper CI limit of the reported pooled point estimate in the meta-analysis for comparison with Mhat.hi.to.reproduce"
label(b2$Data.source) = "How did we get data for this meta-analysis?"
label(b2$Pval.One) = "**One-tailed p-value of study (used for sensitivity analysis)"
label(b2$Pval.Two) = "**Two-tailed p-value of study"
label(b2$signif) = "**Is study two-tailed significant?"
label(b2$affirm) = "**Is study affirmative?"

label(b2$Eta.AllEsts) = "Eta estimate using all point estimates (no random selection); ONLY used for checking interrater reliability and NOT used for analysis"
label(b2$EtaInv.AllEsts) = "Inverse of Eta.AllEsts"
label(b2$LoEtaInv.AllEsts) = "Lower CI limit of Eta.AllEsts from delta method"
label(b2$HiEtaInv.AllEsts) = "Upper CI limit of Eta.AllEsts from delta method"

label(b2$HiEtaInv.AllEsts) = "Upper CI limit of EtaInv.AllEsts directly from selection model"
label(b2$LoEtaInv.AllEsts) = "Lower CI limit of EtaInv.AllEsts directly from selection model"
label(b2$VarEtaInv.AllEsts) = "Variance of EtaInv.AllEsts directly from selection model"

label(b2$LogEta.AllEsts) = "Log of Eta.AllEsts"
label(b2$VarLogEta.AllEsts) = "Variance of LogEta.AllEsts from delta method"
label(b2$LoLogEta.AllEsts) = "Lower CI limit of LogEta.AllEsts from delta method"
label(b2$HiLogEta.AllEsts) = "Upper CI limit of LogEta.AllEsts from delta method"

label(b2$shapiro.pval.AllEsts) = "Shapiro normality test p-value using all estimates"

label(b2$n.coders) = "How many people coded this meta-analysis in duplicate?"
label(b2$coders) = "Initials of people who coded this meta-analysis in duplicate"
label(b2$status) = "{erfect agreement = meta-analysis has no interrater discrepancies on key statistics; slightly discrepant = meta-analysis has <5% max interrater discrepancy on key statistics, so we chose the first coder's dataset;
discrepant = meta-analysis has <5% max interrater discrepancy on key statistics, so we manually resolved the conflict to choose a superseding coder;
singly-coded = meta-analysis was not dual-coded due to time constraints, so was excluded from analysis"
label(b2$use) = "**Should this row be used in analysis (i.e., after resolution of interrater conflicts and removal of duplicated data)?"
label(b2$randomly.chosen) = "**Indicator for whether point estimate was randomly chosen among all point estimates within a particular study and meta-analysis (for use in independence-assuming selection models)"

label(b2$Eta) = "**Eta estimate using randomly-chosen point estimates"
label(b2$LogEta) = "**Log of Eta (the scale we used in meta-analyzing the estimates)"
label(b2$VarLogEta) = "**Variance of LogEta from delta method"
label(b2$LoLogEta) = "Lower CI limit of LogEta from delta method"
label(b2$HiLogEta) = "Upper CI limit of LogEta from delta method"

label(b2$HiEtaInv) = "Upper CI limit of EtaInv directly from selection model"
label(b2$LoEtaInv) = "Lower CI limit of EtaInv directly from selection model"
label(b2$VarEtaInv) = "Variance of EtaInv directly from selection model"
label(b2$EtaInv) = "Inverse of Eta (the scale on which the selection model reports it)"

label(b2$shapiro.pval) = "**Shapiro normality test p-value using randomly-chosen estimates"
label(b2$group.pretty) = "Plotting-friendly recoding of group"
label(b2$Qi) = "**Percentile of point estimate within its meta-analysis"
label(b2$year.rk) = "Chronological rank of paper's publication year, relative to all others in its meta-analysis"
label(b2$n.first.yr) = "Number of unique PAPERS (not point estimates) published in the first year in which any paper(s) were published"
label(b2$n.first2) = "Number of unique PAPERS (not point estimates) published 'early'"
label(b2$first2) = "**Was this point estimate published 'early'? Missing data occur when we did not code the meta-analysis' year due to time constaints on data collection."
label(b2$Sval.Est) = "Sensitivity analysis S-value for the meta-analytic pooled point estimate, Mhat (i.e., based on robust clustered re-analysis).
>200 indicates the S-value is above the highest value in the grid search of 200."
label(b2$Sval.CI) = "Sensitivity analysis S-value for the lower CI limit of the meta-analytic pooled point estimate (i.e., based on robust clustered re-analysis). It is possible for
Sval.CI to indicate complete robustness even if the original CI limit, Mhat.Lo, is already <1. This is because the former uses user-specified weights in robu() via corrected_meta(), whereas
the latter allows robu() to choose its own weights."
label(b2$Sval.Est.Num) = "Numeric version of Sval.Est where 'not possible' has been converted to Inf and >200 has been converted to 200."
label(b2$Sval.CI) = "Numeric version of Sval.CI, similar to Sval.Est.Num"
label(b2$Mhat.Worst) = "**Sensitivity analysis worst-case estimate of meta-analytic pooled point estimate.
Like Mhat, this is based on a robust clustered analysis and is still on the original effect.measure scale, so the two are directly comparable."
label(b2$Mhat.Worst.Lo) = "Lower CI limit of Mhat.Worst"
label(b2$Mhat.Worst.Hi) = "Upper CI limit of Mhat.Worst"
label(b2$Mhat.Worst.Pval) = "P-value of Mhat.Worst"
label(b2$Mhat) = "**Best-practice pooled point estimate of meta-analysis. This pooled estimate uses all point estimates and is based on a robust clustered model
as fit by the function sapb_stats. This estimate has NOT been transformed back to the scale described by effect.measure and instead is presented on the scale
described in Analysis.Scale. This is so that we can always treat 0 as the null."
label(b2$Mhat.Lo) = "Lower CI limit of Mhat"
label(b2$Mhat.Hi) = "Upper CI limit of Mhat"
label(b2$Mhat.Pval) = "P-value of Mhat"
label(b2$Reported.CI.hi) = "Upper limit of Reported.pooled.estimate"

#View(label(b2))
my.names = names(label(b2))
cd = as.data.frame(label(b2))
cd$variable = my.names
names(cd)[ names(cd) == "label(b2)"] = "description"
cd = cd %>% select(variable, description ) # reorder columns for prettiness


# write codebook as csv
setwd(data.dir)
setwd("Prepped data for analysis")
write.csv( cd,
           "codebook_for_b2_data_prepped_step2.csv",
           row.names = FALSE)


##### Label Variables and Make Codebook for f2 Dataset #####
f2 = f2[ , !names(f2) %in% c("X") ]
label(f2$k.all) = "Number of point estimates in meta-analysis (before random selection)"
label(f2$k.rc) = "Number of randomly-selected point estimates in meta-analysis"
label(f2$k.affirm.rc) = "Number of randomly-selected point estimates in meta-analysis that were affirmative"
label(f2$k.nonaffirm.rc) = "Number of randomly-selected point estimates in meta-analysis that were nonaffirmative"

label(f2$Qi.hif) = "Among top-tier point estimates, mean point estimate percentile within its meta-analysis"
label(f2$Qi.lif) = "Among lower-tier point estimates, mean point estimate percentile within its meta-analysis"
label(f2$Qi.hif.diff) = "Qi.hif - Qi.lif"

label(f2$Psig.hif) = "Among top-tier point estimates, probability that p-value is two-tailed 'significant'"
label(f2$Psig.lif) = "Among lower-tier point estimates, probability that p-value is two-tailed 'significant'"
label(f2$Psig.hif.diff) = "Psig.hif - Psig.lif"
label(f2$Psig.hif.RR) = "Psig.hif / Psig.lif"

label(f2$k.hif) = "Number of top-tier point estimates in meta-analysis (before random selection)"
label(f2$k.lif) = "Number of lower-tier point estimates in meta-analysis (before random selection)"

label(f2$Qi.early) = "Among early point estimates, mean point estimate percentile within its meta-analysis"
label(f2$Qi.late) = "Among later point estimates, mean point estimate percentile within its meta-analysis"
label(f2$Qi.early.diff) = "Qi.early - Qi.late"

label(f2$Psig.early) = "Among early point estimates, probability that p-value is two-tailed 'significant'"
label(f2$Psig.late) = "Among later point estimates, probability that p-value is two-tailed 'significant'"
label(f2$Psig.early.diff) = "Psig.early - Psig.late"
label(f2$Psig.early.RR) = "Psig.early / Psig.late"

label(f2$k.early) = "Number of early point estimates in meta-analysis (before random selection)"
label(f2$k.late) = "Number of later point estimates in meta-analysis (before random selection)"


label(f2$Pdisaffirm.ratio) = "Ratio of observed proportion of nonaffirmative p-values above 0.975 to expected number assuming uniform distribution"
label(f2$PZeroEst) = "Probability within this meta-analysis of an exactly zero point estimate"

label(f2$logRR.hif) = "For this meta-analysis, log-relative-risk of point estimate's being affirmative comparing top- to lower-tier studies"
label(f2$varlogRR.hif) = "Variance of logRR.hif"

label(f2$logRR.first2) = "For this meta-analysis, log-relative-risk of point estimate's being affirmative comparing early to later studies"
label(f2$varlogRR.first2) = "Variance of logRR.first2"

label(f2$excl.Pdisaffirm.ratio) = "Should this meta-analysis be excluded in the sensitivity analysis regarding two-tailed selection?"
label(f2$excl.PZeroEst) = "Should this meta-analysis be excluded in the sensitivity analysis regarding point masses of estimates at 0?"
label(f2$excl.shapiro) = "Should this meta-analysis be excluded in the sensitivity analysis regarding non-normality?"
label(f2$excl.other) = "Should this meta-analysis be excluded in the sensitivity analysis regarding other problems?"

#View(label(f2))
my.names = names(label(f2))
cd = as.data.frame(label(f2))
cd$variable = my.names
names(cd)[ names(cd) == "label(f2)"] = "description"
cd = cd %>% select(variable, description ) # reorder columns for prettiness

setwd(data.dir)
setwd("Prepped data for analysis")
write.csv( cd,
           "codebook_for_f2_data_prepped_step2.csv",
           row.names = FALSE)

#################################### WRITE DATASETS #################################### 

f2 = apply(f2, 2, unlist)
setwd(data.dir)
setwd("Prepped data for analysis")
write.csv(f2, "**f2_data_aggregated_step2.csv")

setwd(data.dir)
setwd("Prepped data for analysis")
write.csv(b2, "**b2_data_prepped_step2.csv")

# public version of b2 (not including studies with private data)
setwd(data.dir)
setwd("Prepped data for analysis")
b2.public = b2 %>% filter( Data.source != "author.contact" )
write.csv(b2.public, "**b2_data_prepped_step2_PUBLIC.csv")

